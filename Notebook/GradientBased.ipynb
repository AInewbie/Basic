# DON'T WORRY ABOUT THIS CELL, IT JUST SETS SOME STUFF UP THAT WAS CODED IN PREVIOUS NOTEBOOKS
import numpy as np
import matplotlib.pyplot as plt

def sample_linear_data(m=20):
    ground_truth_w = 2.3 # slope
    ground_truth_b = -8 #intercept
    X = np.random.randn(m)*2
    Y = ground_truth_w*X + ground_truth_b + 0.2*np.random.randn(m)
    return X, Y #returns X (the input) and Y (labels)

def plot_data(X, Y):
    plt.figure()
    plt.scatter(X, Y, c='r')
    plt.xlabel('X')
    plt.ylabel('Y')
    plt.show()

m = 50
X, Y = sample_linear_data(m)
plot_data(X, Y)

def plot_h_vs_y(X, y_hat, Y):
    plt.figure()
    plt.scatter(X, Y, c='r', label='Label')
    plt.scatter(X, y_hat, c='b', label='Hypothesis', marker='x')
    plt.legend()
    plt.xlabel('X')
    plt.ylabel('Y')
    plt.show()

    def L(y_hat, labels):
        errors = y_hat - labels ## calculate errors
        squared_errors = errors ** 2 ## square errors
        mean_squared_error = sum(squared_errors) / len(squared_errors) ## calculate mean
        return mean_squared_error # return loss

    class LinearHypothesis:
        def __init__(self): #initalize parameters
            self.w = np.random.randn() ## randomly initialise weight
            self.b = np.random.randn() ## randomly initialise bias

        def __call__(self, X): # how do we calculate output from an input in our model?
            ypred = self.w * X + self.b ## make a prediction
            return ypred # return prediction

        def update_params(self, new_w, new_b):
            self.w = new_w ## set this instance's weights to the new weight value passed to the function
            self.b = new_b ## do the same for the bias


H = LinearHypothesis() # instantiate our linear model
y_hat = H(X) # make prediction
print('Input:',X, '\n')
print('W:', H.w, 'B:', H.b, '\n')
print('Prediction:', y_hat, '\n')

def plot_h_vs_y(X, y_hat, Y):
    plt.figure()
    plt.scatter(X, Y, c='r', label='Label')
    plt.scatter(X, y_hat, c='b', label='Hypothesis', marker='x')
    plt.legend()
    plt.xlabel('X')
    plt.ylabel('Y')
    plt.show()

    plot_h_vs_y(X, y_hat, Y)

    cost = L(y_hat, Y)
    print(cost)

    def random_search(n_samples, limit=20):
    best_weights = np.random.uniform(-limit, limit) # no best weight found yet
    best_bias = np.random.uniform(-limit, limit) # no best bias found yet
    lowest_cost = float('inf') # initialize it very high
    for i in range(0, n_samples): # try this many different parameterisations
        w = np.random.uniform(-limit, limit) # randomly sample a weight within the limits of the search
        b = np.random.uniform(-limit, limit) # randomly sample a bias within the limits of the search
        H.update_params(w, b) # update our model with random parameters
        y_hat = H(X) # make prediction
        cost = L(y_hat, Y) # calculate loss
        if cost < lowest_cost: # if this is the best parameterisation so far
            lowest_cost = cost # update the lowest running cost to the cost for this parameterisation
            best_weights = w # get best weights so far from the model
            best_bias = b # get best bias so far from the model
    print('Lowest cost of', lowest_cost, 'achieved with weight of', best_weights, 'and bias of', best_bias)
    return best_weights, best_bias



best_weights, best_bias = random_search(100) # do 100 samples in a random search
H.update_params(best_weights, best_bias) # make sure to set our model's weights to the best values we found
plot_h_vs_y(X, H(X), Y)


from itertools import permutations
def generate_grid_search_values(n_params, n_samples=100, minval=-2.5, maxval=2.5):
    n_samples_per_param = int(np.power(n_samples, 1 / n_params)) # want 100 samples for 2 variables, so try 10 different values for each parameter because 10^2=100
    print(f'Trying {n_samples_per_param} samples per parameter')
    param_values = np.linspace(minval, maxval, n_samples_per_param) ## get list of different parameters to try
    grid_samples = permutations(param_values, n_params) ## try every possible permutation of the param values
    return grid_samples

def grid_search(grid_search_values):
    best_weights = np.random.randn() ## no best weight found yet
    best_bias = np.random.randn() ## no best bias found yet
    lowest_cost = float('inf') ## initialize it very high
    for search_val in grid_search_values: # for each model parameterisation that we will try
        w, b = search_val
        H.update_params(w, b) ## update model parameters
        y_hat = H(X) ## make prediction
        cost = L(y_hat, Y) ## calculate loss
        if cost < lowest_cost: ## if this is the best parameterisation so far
            lowest_cost = cost ## update the lowest running cost to the cost for this parameterisation
            best_weights = w ## get best weights so far from the model
            best_bias = b ## get best bias so far from the model
    print('Lowest cost of', lowest_cost, 'achieved with weight of', best_weights, 'and bias of', best_bias)
    return best_weights, best_bias

    # try using 100 grid search values
grid_search_values = generate_grid_search_values(2, n_samples=100) # generate model parameterisations to try
best_weights, best_bias = grid_search(grid_search_values) # perform grid search
H.update_params(best_weights, best_bias) ## update model with best parameters found
plot_h_vs_y(X, H(X), Y) # plot predictions and true labels

grid_search_values = generate_grid_search_values(2, n_samples=99) # generate model parameterisations to try
best_weights, best_bias = grid_search(grid_search_values) # perform grid search
H.update_params(best_weights, best_bias) # update model with best parameters found
plot_h_vs_y(X, H(X), Y) # plot predictions and true labels
